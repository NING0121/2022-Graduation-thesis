{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from Utils import VariantWordDataset, Config\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化全局参数 Config 对象\n",
    "config = Config()\n",
    "\n",
    "\n",
    "# 构建数据集\n",
    "train_set = VariantWordDataset(\"train\", config.source_dic_path, config.target_dic_path)\n",
    "valid_set = VariantWordDataset(\"test\", config.source_dic_path, config.target_dic_path)\n",
    "print(f\"Train size: {len(train_set)}\")\n",
    "\n",
    "# 构建data_loader\n",
    "n_cpu = os.cpu_count()\n",
    "train_dataloader = DataLoader(train_set, batch_size=config.batch_size, shuffle=True, collate_fn=train_set.generate_batch, num_workers=n_cpu)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=config.batch_size, shuffle=False, collate_fn=valid_set.generate_batch, num_workers=n_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model.BaselineModel import BaselineModel\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# import wandb\n",
    "\n",
    "# 初始化 wandb\n",
    "# wandb.init(project=\"Graduation_project\")\n",
    "\n",
    "# 模型初始化\n",
    "model = BaselineModel(config)\n",
    "\n",
    "# # wandb logger\n",
    "# wandb_logger = WandbLogger(project = \"Graduation_project\",\n",
    "#                            name = 'Transformer-CrossEL-lr-0.02',\n",
    "#                            save_dir = '../Logs',\n",
    "#                            log_model=\"all\")\n",
    "\n",
    "# # 模型参数保存\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     monitor=\"valid_accuracy\",\n",
    "#     dirpath=\"../Weights\",\n",
    "#     filename=\"Baseline-Transformer-CrossEntropyLoss-{epoch:02d}-{valid_accuracy:.2f}\",\n",
    "#     save_top_k=3,\n",
    "#     mode=\"max\",\n",
    "# )\n",
    "\n",
    "# # 训练 Trainer 定义\n",
    "# trainer = pl.Trainer(\n",
    "#     max_epochs=5, \n",
    "#     gpus=0,\n",
    "#     logger = wandb_logger,\n",
    "#     callbacks=[checkpoint_callback]\n",
    "#     )\n",
    "\n",
    "\n",
    "# # 模型训练\n",
    "# trainer.fit(\n",
    "#     model, \n",
    "#     train_dataloaders=train_dataloader, \n",
    "#     val_dataloaders=valid_dataloader\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"..\")\n",
    "from Model.ConvS2SModel import ConvS2SModel\n",
    "from Utils.Variant_word import VariantWordDataset\n",
    "import torch\n",
    "from Utils.config import Config\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "# def translate(model, src, data_loader, config):\n",
    "    \n",
    "#     source_dic = data_loader.source_dic    \n",
    "#     target_dic = data_loader.target_dic\n",
    "\n",
    "#     model.eval()\n",
    "\n",
    "#     tokens = [source_dic.word2idx[i] for i in list(src)] # 构造一个样本\n",
    "#     num_tokens = len(tokens)\n",
    "#     src = (torch.LongTensor(tokens).reshape(num_tokens, 1))  # 将src_len 作为第一个维度\n",
    "#     tgt_tokens = greedy_decode(model, src, max_len=num_tokens + 5,\n",
    "#                                 start_symbol=config.BOS_IDX, config=config).flatten()  # 解码的预测结果\n",
    "    \n",
    "#     return \" \".join([target_dic.idx2word[int(tok)] for tok in tgt_tokens]).replace(\"[BOS]\", \"\").replace(\"[EOS]\", \"\")\n",
    "\n",
    "\n",
    "# def greedy_decode(model, src, max_len, start_symbol, config):\n",
    "\n",
    "#     src = src.to(config.device)\n",
    "#     memory = model.encoder(src)  # 对输入的Token序列进行解码翻译\n",
    "#     ys = torch.ones(1, 1).fill_(start_symbol). \\\n",
    "#        type(torch.long).to(config.device)  # 解码的第一个输入，起始符号\n",
    "\n",
    "#     for i in range(max_len - 1):\n",
    "#         memory = memory.to(config.device)\n",
    "#         tgt_mask = (model.my_transformer.generate_square_subsequent_mask(ys.size(0))\n",
    "#                    .type(torch.bool)).to(config.device)  # 根据tgt_len产生一个注意力mask矩阵（对称的）\n",
    "#         out = model.decoder(ys, memory, tgt_mask)  # [tgt_len,tgt_vocab_size]\n",
    "#         out = out.transpose(0, 1)  # [tgt_vocab_size, tgt_len]\n",
    "#         prob = model.classification(out[:, -1])  # 只对对预测的下一个词进行分类\n",
    "#         _, next_word = torch.max(prob, dim=1)  # 选择概率最大者\n",
    "#         next_word = next_word.item()\n",
    "#         ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "#         # 将当前时刻解码的预测输出结果，同之前所有的结果堆叠作为输入再去预测下一个词。\n",
    "#         if next_word == config.EOS_IDX:  # 如果当前时刻的预测输出为结束标志，则跳出循环结束预测。\n",
    "#             break\n",
    "#     return ys\n",
    "\n",
    "# def translate_to_right(src, config):\n",
    "#     data_loader = VariantWordDataset(\"train\", config.source_dic_path, config.target_dic_path)\n",
    "#     translation_model = ConvS2SModel(config)\n",
    "#     translation_model = translation_model.to(config.device)\n",
    "#     torch.load(\"../../Weights_2/ConvS2SModel-CrossEntropyLoss-epoch=11-valid_f1=0.97.ckpt\", map_location=\"cpu\")\n",
    "#     r = translate(translation_model, src, data_loader, config)\n",
    "#     return r\n",
    "\n",
    "\n",
    "config = Config()\n",
    "translation_model = ConvS2SModel(config)\n",
    "translation_model = translation_model.to(config.device)\n",
    "torch.load(\"../../Weights_2/ConvS2SModel-CrossEntropyLoss-epoch=11-valid_f1=0.97.ckpt\", map_location=\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "srcs = [\"9306你好,鉴于你良好的信誉,特聘请你~来我店帮忙工作（350/天）咨询,Q:707941883.\"]\n",
    "tgts = [\"9306你好,鉴于你良好的信誉,特聘请你来我店帮忙工作(350天)咨询,Q:707941883.\"]\n",
    "for i, src in enumerate(srcs):\n",
    "    r = translate_to_right(src, config)\n",
    "    print(f\"德语：{src}\")\n",
    "    print(f\"翻译：{r}\")\n",
    "    print(f\"英语：{tgts[i]}\")\n",
    "    # print(len([src]))\n",
    "    # print(len([tgts[i]]))\n",
    "    # print([tgts[i]])\n",
    "    print([[i for i in src]])\n",
    "    print([[[i for i in tgts[i]]]])\n",
    "    print(bleu_score([[i for i in src]], [[[i for i in tgts[i]]]]))\n",
    "\n",
    "srcs =  tokens = [source_dic.word2idx[i] for i in list(src)]\n",
    "\n",
    "translation_model(srcs, tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"..\")\n",
    "from Model.ConvS2SModel import ConvS2SModel\n",
    "from Utils.Variant_word import VariantWordDataset\n",
    "from Utils.Dictionary import Dictionary\n",
    "import torch\n",
    "from Utils.config import Config\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# 字典加载\n",
    "source_dic = Dictionary.load_from_file(\"../Data/source_vocal.pkl\")\n",
    "target_dic = Dictionary.load_from_file(\"../Data/target_vocal.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "# 模型载入\n",
    "# translation_model = ConvS2SModel(config)\n",
    "# translation_model = translation_model.to(\"cpu\")\n",
    "# torch.load(\"../../Weights_2/ConvS2SModel-CrossEntropyLoss-epoch=11-valid_f1=0.97.ckpt\", map_location=\"cpu\")\n",
    "\n",
    "\n",
    "# 测试句子\n",
    "srcs = [\"9306你好,鉴于你良好的信誉,特聘请你~来我店帮忙工作（350/天）咨询,Q:707941883.,特聘请你~来我店帮忙工作（350/天）咨询,Q:707941883.\"]\n",
    "tgts = [\"9306你好,鉴于你良好的信誉,特聘请你来我店帮忙工作(350天)咨询,Q:707941883.9306你好,鉴于你良好的信誉,特聘请你来我店帮忙工作(350天)咨询,Q:707941883.\"]\n",
    "\n",
    "\n",
    "# src = torch.LongTensor([ source_dic.word2idx[i] for i in srcs[0] ]).unsqueeze(0).to(device=\"cpu\")\n",
    "# tgt = torch.LongTensor([ source_dic.word2idx[i] for i in tgts[0] ]).unsqueeze(0).to(device=\"cpu\")\n",
    "\n",
    "# src_length = torch.LongTensor(src.size(0))\n",
    "# type(src_length)\n",
    "\n",
    "\n",
    "\n",
    "for i, src in enumerate(srcs):\n",
    "    print(f\"德语：{srcs}\")\n",
    "    print(f\"英语：{tgts[i]}\")\n",
    "    # print(len([src]))\n",
    "    # print(len([tgts[i]]))\n",
    "    # print([tgts[i]])\n",
    "    print([[i for i in src]])\n",
    "    print([[[i for i in tgts[i]]]])\n",
    "    print(bleu_score([[i for i in src]], [[[i for i in tgts[i]]]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dictionary from ../Data/source_vocal.pkl\n",
      "loading dictionary from ../Data/target_vocal.pkl\n",
      "loading dictionary from ../Data/source_vocal.pkl\n",
      "loading dictionary from ../Data/target_vocal.pkl\n",
      "loading dictionary from ../Data/source_vocal.pkl\n",
      "loading dictionary from ../Data/target_vocal.pkl\n",
      "loading dictionary from ../Data/source_vocal.pkl\n",
      "loading dictionary from ../Data/target_vocal.pkl\n",
      "loading dictionary from ../Data/source_vocal.pkl\n",
      "loading dictionary from ../Data/target_vocal.pkl\n",
      "Train size: 6815\n"
     ]
    }
   ],
   "source": [
    "# 翻译测试\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\".\")\n",
    "from Model.ConvS2SModel import ConvS2SModel\n",
    "from Model.TransformerModel import TransformerModel\n",
    "from Model.RNNSearchModel import RNNSearchModel\n",
    "from Utils.Variant_word import VariantWordDataset\n",
    "import torch\n",
    "from Utils.config import Config\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from Utils import VariantWordDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn,Tensor\n",
    "from Utils.config import Config\n",
    "from Model import *\n",
    "import wandb\n",
    "\n",
    "# 模型加载\n",
    "config = Config()\n",
    "config.source_dic_path = \"../Data/source_vocal.pkl\"\n",
    "config.target_dic_path = \"../Data/target_vocal.pkl\"\n",
    "config.src2tgt_path = \"../Data/src_idx2tgt_idx.pkl\"\n",
    "config.train_set_path =  '../Data/Dataset/train_data.csv'\n",
    "config.test_set_path =  '../Data/Dataset/test_data.csv'\n",
    "config.train_set_supply_path =  '../Data/Dataset/train_data_supply_0417.csv'\n",
    "# config.device =  'cpu'\n",
    "config.d_model = 256\n",
    "config.num_head = 8\n",
    "config.num_encoder_layers = 3\n",
    "config.num_decoder_layers = 3\n",
    "config.dim_feedforward = 512\n",
    "\n",
    "\n",
    "convs2s = ConvS2SModel(config)\n",
    "convs2s.to(config.device).load_state_dict(torch.load(\"../Weights_ConvS2SModel-CrossEntropyLoss/ConvS2SModel-CrossEntropyLoss-epoch=33-valid_f1=0.98.ckpt\")[\"state_dict\"])\n",
    "rnnsearch = RNNSearchModel(config)\n",
    "rnnsearch.to(config.device).load_state_dict(torch.load(\"../Weights_RNNsearchModel-CrossEntropyLoss/RNNsearchModel-CrossEntropyLoss-epoch=34-valid_f1=0.91.ckpt\")[\"state_dict\"])\n",
    "transformer = TransformerModel(config)\n",
    "transformer.to(config.device).load_state_dict(torch.load(\"../Weights_TransformerModel-CrossEntropyLoss/TransformerModel-CrossEntropyLoss-epoch=34-valid_f1=0.94.ckpt\")[\"state_dict\"])\n",
    "\n",
    "\n",
    "# 数据集构建\n",
    "train_set = VariantWordDataset(\"train\", config, isAligned=False, supply_ratio=0)\n",
    "valid_set = VariantWordDataset(\"test\", config, isAligned=False)\n",
    "print(f\"Train size: {len(train_set)}\")\n",
    "\n",
    "\n",
    "# dataloader 初始化\n",
    "# 数据传输cpu数目\n",
    "n_cpu = os.cpu_count()\n",
    "train_dataloader = DataLoader(train_set, batch_size=1, shuffle=True, collate_fn=train_set.generate_batch, num_workers=n_cpu)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=1, shuffle=False, collate_fn=valid_set.generate_batch, num_workers=n_cpu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def tran_ConvS2S(config, src, tgt, tgt_length, isAligned):\n",
    "\n",
    "    start =time.clock()\n",
    "    src = src.to(config.device).transpose(0,1)  # [ batch_size, src_len ]\n",
    "    tgt = tgt.to(config.device).transpose(0,1)  # [ batch_size, tgt_len ]\n",
    "    src_length = src.size(0)  # [ batch_size ]\n",
    "    tgt_input = tgt[:, 1:-1]  # 解码部分的输入, [ batch_size, tgt_len ]\n",
    "    decoder_out, lprobs  = convs2s(src, src_length, tgt_input)\n",
    "    \n",
    "\n",
    "    tgt_input = tgt_input.reshape(-1)\n",
    "    decoder_out = decoder_out[:,:,:].argmax(axis=2).reshape(-1)\n",
    "    print(decoder_out.device)\n",
    "    decoder_out = np.delete(Tensor.cpu(decoder_out).numpy() , np.where(Tensor.cpu(tgt_input).numpy() <= config.GAP_IDX))\n",
    "    tgt_input = np.delete(Tensor.cpu(tgt_input).numpy() , np.where(Tensor.cpu(tgt_input).numpy() <= config.GAP_IDX))\n",
    "\n",
    "    end = time.clock()\n",
    "\n",
    "    candidate = [str(train_set.target_dic.idx2word[i]) for i in decoder_out.tolist()]\n",
    "    reference = [str(train_set.target_dic.idx2word[i]) for i in tgt_input.tolist()]\n",
    "    print(\"\".join(candidate))\n",
    "    print(\"\".join(reference))\n",
    "    print(bleu_score([candidate], [[reference]]))\n",
    "    print( end )\n",
    "    print( start )\n",
    "    \n",
    "    return bleu_score([candidate], [[reference]]), end - start\n",
    "\n",
    "\n",
    "def create_mask(vector, PAD_IDX):\n",
    "\n",
    "    vector_mask = (vector != PAD_IDX)\n",
    "    return vector_mask\n",
    "\n",
    "def traverse(tensor,  PAD_IDX, tgt_lengths):\n",
    "    # 先去除所有的PAD\n",
    "    new_tensor = []\n",
    "\n",
    "    for i, length in zip(tensor, tgt_lengths.cpu()):\n",
    "        i = i[:length]\n",
    "            # 倒序\n",
    "        new_tensor.append( i.flip(0) )\n",
    "\n",
    "    # PAD\n",
    "    tensor = pad_sequence(new_tensor, padding_value=PAD_IDX, batch_first=True)  # [de_len,batch_size]\n",
    "\n",
    "    return tensor\n",
    "\n",
    "def tran_RNNSearch(config, src, tgt, tgt_length, isAligned):\n",
    "\n",
    "    start =time.clock()\n",
    "\n",
    "    src = src.to(config.device).transpose(0, 1) # [batch_size, src_len]\n",
    "    tgt = tgt.to(config.device).transpose(0, 1)\n",
    "    a = np.delete(Tensor.cpu(tgt).numpy() , np.where(Tensor.cpu(tgt).numpy() <= config.GAP_IDX))\n",
    "    print(a)\n",
    "    reference = [str(train_set.target_dic.idx2word[i]) for i in a.tolist()]\n",
    "    print(\"\".join(reference))\n",
    "    forward_tgt = tgt\n",
    "    backward_tgt = traverse(tgt, config.PAD_IDX, tgt_length)\n",
    "\n",
    "    src_mask = create_mask(src, config.PAD_IDX)\n",
    "    forward_tgt_mask = create_mask(forward_tgt, config.PAD_IDX)\n",
    "    backward_tgt_mask = create_mask(backward_tgt, config.PAD_IDX)\n",
    "\n",
    "        \n",
    "    # logits 输出shape为[tgt_len,batch_size,tgt_vocab_size]\n",
    "    loss, w_loss, output = rnnsearch(\n",
    "        src = src,                   # Encoder的token序列输入，[src_len,batch_size]\n",
    "        src_mask = src_mask, \n",
    "        f_trg = forward_tgt, \n",
    "        f_trg_mask = forward_tgt_mask,\n",
    "        b_trg=backward_tgt, \n",
    "        b_trg_mask=backward_tgt_mask)\n",
    "\n",
    "    output = output[:,1:-1].reshape(-1)\n",
    "    forward_tgt = forward_tgt[:,1:-1].reshape(-1)\n",
    "    print(forward_tgt.shape)\n",
    "    print(output.shape)\n",
    "\n",
    "    output = np.delete(Tensor.cpu(output).numpy() , np.where(Tensor.cpu(forward_tgt).numpy() <= config.GAP_IDX))\n",
    "    forward_tgt = np.delete(Tensor.cpu(forward_tgt).numpy() , np.where(Tensor.cpu(forward_tgt).numpy() <= config.GAP_IDX))\n",
    "\n",
    "    end = time.clock()\n",
    "\n",
    "    candidate = [str(train_set.target_dic.idx2word[i]) for i in output.tolist()]\n",
    "    reference = [str(train_set.target_dic.idx2word[i]) for i in forward_tgt.tolist()]\n",
    "    print(\"\".join(candidate))\n",
    "    print(\"\".join(reference))\n",
    "    # print(bleu_score([candidate], [[reference]]))\n",
    "    return bleu_score([candidate], [[reference]]), end - start\n",
    "\n",
    "def generate_square_subsequent_mask(config, sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=config.device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask_1(config, src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(config, tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=config.device).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == config.PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == config.PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "def tran_transformer(config, src, tgt, tgt_length, _):\n",
    "\n",
    "    start =time.clock()\n",
    "    src = src.cuda()\n",
    "    tgt = tgt.cuda()\n",
    "\n",
    "    tgt_input = tgt[:-1, :]  # 解码部分的输入, [tgt_len,batch_size]\n",
    "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask \\\n",
    "            = create_mask_1(config, src, tgt_input)\n",
    "    \n",
    "    # logits 输出shape为[tgt_len,batch_size,tgt_vocab_size]\n",
    "    logits = transformer(\n",
    "                            src=src,  # Encoder的token序列输入，[src_len,batch_size]\n",
    "                            tgt=tgt_input,  # Decoder的token序列输入,[tgt_len,batch_size]\n",
    "                            src_mask=src_mask,  # Encoder的注意力Mask输入，这部分其实对于Encoder来说是没有用的\n",
    "                            tgt_mask=tgt_mask, # Decoder的注意力Mask输入，用于掩盖当前position之后的position [tgt_len,tgt_len]\n",
    "                            src_key_padding_mask=src_padding_mask,  # 用于mask掉Encoder的Token序列中的padding部分\n",
    "                            tgt_key_padding_mask=tgt_padding_mask,  # 用于mask掉Decoder的Token序列中的padding部分\n",
    "                            memory_key_padding_mask=src_padding_mask)  # 用于mask掉Encoder的Token序列中的padding部分\n",
    "\n",
    "\n",
    "    ### 计算loss\n",
    "    tgt_out = tgt[1:, :]  # 解码部分的真实值  shape: [tgt_len,batch_size]\n",
    "    # loss = self.loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "\n",
    "\n",
    "    decoder_out = np.delete(Tensor.cpu( logits.reshape(-1, logits.shape[-1]).argmax(1)).detach().numpy() , np.where(Tensor.cpu( logits.reshape(-1, logits.shape[-1]).argmax(1)).detach().numpy()  <= config.GAP_IDX))\n",
    "    tgt_out = np.delete(Tensor.cpu(tgt_out).numpy() , np.where(Tensor.cpu(tgt_out).numpy() <= config.GAP_IDX))\n",
    "\n",
    "    end = time.clock()\n",
    "\n",
    "    candidate = [str(train_set.target_dic.idx2word[i]) for i in decoder_out.tolist()]\n",
    "    reference = [str(train_set.target_dic.idx2word[i]) for i in tgt_out.tolist()]\n",
    "    print(\"\".join(candidate))\n",
    "    print(\"\".join(reference))\n",
    "    print(bleu_score([candidate], [[reference]]))\n",
    "    # print(end - start)\n",
    "    return bleu_score([candidate], [[reference]]), end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "赖晚开出(46虎),您中了吗?加微:18683367080我可以告诉你下次的一肖三码,早加就知道\n",
      "昨晚开出(46虎),您中了吗?加微:18683367080我可以告诉你下次的一肖三码,早加就知道\n",
      "0.9784820675849915\n",
      "354.695067\n",
      "354.659237\n",
      "昨晚开出(46虎),您中了?wei:18683367080我可可告诉你下次的一肖三码,早加\n",
      "晚开出(46虎),您中了吗?加微:18683367080我可以告诉你下次的一肖三码,早加就知道\n",
      "0.7600666967065693\n",
      "[3188 3165 1781 2526  262  479  716  109 1086 3283 1881   44 3279 1435\n",
      " 1054 1419  351  453  748 3359  716 3359 2927 2927  716 1587 1480 3359\n",
      " 1480 2440 3081  688 2936  711 2587 1495  317 1711 3185  886 1598 1138\n",
      " 3283  169 1419 1504  318 2824]\n",
      "昨晚开出(46虎),您中了吗?加微:18683367080我可以告诉你下次的一肖三码,早加就知道\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "昨晚开出(46[GAP]机,您中了解?加wei6833677080[GAP]我可以告你你次次一肖三码,早加就知道\n",
      "昨晚开出(46虎),您中了吗?加微:18683367080我可以告诉你下次的一肖三码,早加就知道\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/NING/wnn/lib/python3.6/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "../Model_parts/RNNSearch.py:60: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:963.)\n",
      "  logit.data.masked_fill_(1 - mask, -float('inf'))\n",
      "/data/NING/wnn/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.035829999999975826\n",
      "0.011779999999987467\n",
      "0.08268199999997705\n"
     ]
    }
   ],
   "source": [
    "from torch import nn,Tensor\n",
    "import time\n",
    "time1 =float(0)\n",
    "time2 = float(0)\n",
    "time3 = float(0)\n",
    "times = 0\n",
    "# 计算ConvS2S模型时间\n",
    "for index, (src, tgt, tgt_length, _) in enumerate(valid_dataloader):\n",
    "\n",
    "    if index < 1:\n",
    "        bleu1, timec = tran_ConvS2S(config, src, tgt, tgt_length, _)\n",
    "        bleu1, timet = tran_transformer(config, src, tgt, tgt_length, _)\n",
    "        bleu1, timer = tran_RNNSearch(config, src, tgt, tgt_length, _)\n",
    "\n",
    "        time1+= timec\n",
    "        time2 += timet\n",
    "        time3 += timer\n",
    "        times += 1\n",
    "    else:\n",
    "        break\n",
    "print(times)\n",
    "print(time1/times)\n",
    "print(time2/times)\n",
    "print(time3/times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00023599999997259147"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"昨晚开出(46虎),您中了吗?加微:18683367080我可以告诉你下次的一肖三码,早加就知道\"\n",
    "list_1 = list(string)\n",
    "list_2 = []\n",
    "\n",
    "start = time.clock()\n",
    "for i in list_1:\n",
    "    list_2.append(train_set.target_dic.word2idx[i])\n",
    "end = time.clock()\n",
    "end - start"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e14cee0ee6a83057731cf0853bdd17c3d89c7b62da788446c49af076e5ce1dd4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
