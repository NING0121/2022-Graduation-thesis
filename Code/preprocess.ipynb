{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 根据语料分别构建源数据词表 和 目标数据词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "from Utils.Dictionary import Dictionary\n",
    "\n",
    "\n",
    "data_path = \"../Data/Dataset/data.csv\"\n",
    "source_dic_path = \"../Data/source_vocal.pkl\"\n",
    "target_dic_path = \"../Data/target_vocal.pkl\"\n",
    "\n",
    "\n",
    "# 生成 源数据字典 and 目标数据字典\n",
    "source_dic = Dictionary()\n",
    "source_dic.load_from_data(data_path, \"raw_data\")\n",
    "source_dic.dump_to_file(source_dic_path)\n",
    "\n",
    "target_dic = Dictionary()\n",
    "target_dic.load_from_data(data_path, \"raw_data\")\n",
    "target_dic.dump_to_file(target_dic_path)\n",
    "target_dic.word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 数据集的划分（如果已经划分好就不必划分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 文件路径\n",
    "data_path = \"../Data/Dataset/data.csv\"\n",
    "train_data_path = \"../Data/Dataset/train_data_1.csv\"\n",
    "test_data_path = \"../Data/Dataset/test_data_1.csv\"\n",
    "\n",
    "# 数据集划分\n",
    "split_ratio = 0.6\n",
    "dataset = pd.read_csv(data_path, index_col=0)\n",
    "train_set = dataset.sample(frac = split_ratio, random_state=0, axis=0)\n",
    "test_set = dataset[~dataset.index.isin(train_set.index)]\n",
    "\n",
    "# 数据集保存\n",
    "train_set.to_csv(train_data_path)\n",
    "test_set.to_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from Utils.VariantNeedleman_Wunsch import VariantNW\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "\n",
    "# 寻找可疑变异\n",
    "def findVariant(seq_1, seq_2):\n",
    "    variant_dic = {}\n",
    "    for variant, right in zip(seq_1, seq_2):\n",
    "        \n",
    "        if isVariant(variant, right):\n",
    "            if right not in variant_dic.keys():\n",
    "                value = [variant]\n",
    "                variant_dic[right] = value\n",
    "            else:\n",
    "                value = variant_dic[right]\n",
    "                if variant not in value:\n",
    "                    value.append(variant)\n",
    "                variant_dic[right] = value\n",
    "\n",
    "    return variant_dic\n",
    "\n",
    "def addVariant(dic1, dic2):\n",
    "    for k,v in dic2.items():\n",
    "        if k in dic1.keys():\n",
    "            for i in v:\n",
    "                if i not in dic1[k]:\n",
    "                    dic1[k].append(i)\n",
    "        else:\n",
    "            dic1[k] = v\n",
    "    return dic1\n",
    "\n",
    "def isVariant(char1, char2):\n",
    "    if char1 == char2 or char1 == '[GAP]' or char2 == '[GAP]':\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# 筛选变体字典\n",
    "def filterVariant(variant_dic, remove_AandD = True, remove_single = True, remove_ratio = 0.5):\n",
    "    variant_dic_2 = variant_dic.copy()\n",
    "\n",
    "    for key, value in variant_dic.items():\n",
    "        delete_key = False\n",
    "        \n",
    "        if (key.encode(\"utf-8\").isalpha() or key.isdigit()) and remove_AandD:\n",
    "            delete_key = True\n",
    "\n",
    "        if len(value) == 1 and remove_single:\n",
    "            delete_key = True\n",
    "        \n",
    "        if delete_key:\n",
    "            del variant_dic_2[key]\n",
    "        else:\n",
    "            for val in value:\n",
    "                variantNW.set_seqs(key, val)\n",
    "                variantNW.propagate()\n",
    "                aligned_seq1, aligned_seq2 = variantNW.traceback()\n",
    "                score = variantNW.get_aligned_seq_score(aligned_seq1, aligned_seq2).pop()\n",
    "                if score < remove_ratio:\n",
    "                    variant_dic_2[key].remove(val)\n",
    "                \n",
    "    return variant_dic_2\n",
    "\n",
    "data = pd.read_csv(\"../Data/Dataset/train_data.csv\", index_col=0).reset_index(drop=True)\n",
    "variantNW = VariantNW()\n",
    "\n",
    "variant_dic = {}\n",
    "for row in data.iterrows():\n",
    "    seq1 = row[1][0]\n",
    "    seq2 = row[1][1]\n",
    "    variantNW.set_seqs(seq1, seq2)\n",
    "    variantNW.propagate()\n",
    "    aligned_seq1, aligned_seq2 = variantNW.traceback()\n",
    "    dic = findVariant(aligned_seq1, aligned_seq2)\n",
    "    variant_dic = addVariant(variant_dic, dic)\n",
    "\n",
    "variant_dic = filterVariant(variant_dic)\n",
    "variant_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_dic = filterVariant(variant_dic)\n",
    "variant_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选变体字典\n",
    "def filterVariant(variant_dic, remove_AandD = True, remove_single = True, remove_ratio = 0.5):\n",
    "    variant_dic_2 = variant_dic.copy()\n",
    "\n",
    "    for key, value in variant_dic.items():\n",
    "        delete_key = False\n",
    "        \n",
    "        if (key.encode(\"utf-8\").isalpha() or key.isdigit()) and remove_AandD:\n",
    "            delete_key = True\n",
    "\n",
    "        if len(value) == 1 and remove_single:\n",
    "            delete_key = True\n",
    "        \n",
    "        if delete_key:\n",
    "            del variant_dic_2[key]\n",
    "        else:\n",
    "            for val in value:\n",
    "                variantNW.set_seqs(key, val)\n",
    "                variantNW.propagate()\n",
    "                aligned_seq1, aligned_seq2 = variantNW.traceback()\n",
    "                score = variantNW.get_aligned_seq_score(aligned_seq1, aligned_seq2).pop()\n",
    "                if score < remove_ratio:\n",
    "                    variant_dic_2[key].remove(val)\n",
    "                \n",
    "    return variant_dic_2\n",
    "variant_dic = filterVariant(variant_dic)\n",
    "variant_dic\n",
    "for k,v in variant_dic.items():\n",
    "    if len(v) == 1:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_dic = filterVariant(variant_dic)\n",
    "variant_dic\n",
    "with open(\"../Data/variant_dic_remove_re.json\", \"w\", encoding='UTF-8') as f:\n",
    "    json_str = json.dumps(variant_dic, indent=4, ensure_ascii=False)\n",
    "    f.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from Utils.VariantNeedleman_Wunsch import VariantNW\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# 判断是否是变异\n",
    "def isVariant(char1, char2):\n",
    "    if char1 == char2 or char1 == '[GAP]' or char2 == '[GAP]':\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# 生成变异数据\n",
    "def genVariant(variant_dic, seq_1, seq_2):\n",
    "    \n",
    "    raw_seq_1 = seq_1.copy()\n",
    "\n",
    "    for variant, right in zip(seq_1, seq_2):\n",
    "        \n",
    "        if isVariant(variant, right):\n",
    "            if right in variant_dic.keys():\n",
    "                a = random.choice(variant_dic[right])\n",
    "                while a == variant:\n",
    "                    a = random.choice(variant_dic[right])\n",
    "                    print(a)\n",
    "\n",
    "                seq_1[seq_1.index(variant)] = a\n",
    "    if seq_1 == raw_seq_1:\n",
    "        return 0, 0\n",
    "    else:\n",
    "        return seq_1, seq_2\n",
    "\n",
    "\n",
    "variantNW = VariantNW()\n",
    "# 读取数据\n",
    "data = pd.read_csv(\"../Data/Dataset/train_data.csv\", index_col=0 ).reset_index(drop=True)\n",
    "with open(\"../Data/variant_dic_remove_re.json\", \"r\", encoding='UTF-8') as f:\n",
    "    variant_dic = json.load(f)\n",
    "\n",
    "\n",
    "raw_data = []\n",
    "right_data = []\n",
    "for row in data.iterrows():\n",
    "    seq1 = row[1][0]\n",
    "    seq2 = row[1][1]\n",
    "    variantNW.set_seqs(seq1, seq2)\n",
    "    variantNW.propagate()\n",
    "    aligned_seq1, aligned_seq2 = variantNW.traceback()\n",
    "    seq1,seq2 = genVariant(variant_dic, aligned_seq1, aligned_seq2)\n",
    "\n",
    "    if seq1 == 0:\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            seq1 = \"\".join(seq1).replace(\"[GAP]\",\"\")\n",
    "            seq2 = \"\".join(seq2).replace(\"[GAP]\",\"\")\n",
    "        except:\n",
    "            pass\n",
    "        if row[0] % 500 == 499:\n",
    "            print(\"run\")\n",
    "        raw_data.append(seq1)\n",
    "        right_data.append(seq2)\n",
    "        # print(seq2)\n",
    "\n",
    "dataframe = pd.DataFrame({\"raw_data\":raw_data, \"right_data\":right_data})\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv(\"../Data/Dataset/train_data_supply_0417.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from Model_parts import RNNSearch,ConvS2S, TranslationModel\n",
    "from Utils.config import Config\n",
    "config = Config()\n",
    "r = RNNSearch(config)\n",
    "c = ConvS2S(config)\n",
    "t= TranslationModel(src_vocab_size=config.source_vocab_size,\n",
    "                                         tgt_vocab_size=config.target_vocab_size,\n",
    "                                         d_model=config.d_model,\n",
    "                                         nhead=config.num_head,\n",
    "                                         num_encoder_layers=config.num_encoder_layers,\n",
    "                                         num_decoder_layers=config.num_decoder_layers,\n",
    "                                         dim_feedforward=config.dim_feedforward,\n",
    "                                         dropout=config.dropout)\n",
    "def get_model_size(model):\n",
    "\tpara_num = sum([p.numel() for p in model.parameters()])\n",
    "\t# para_size: 参数个数 * 每个4字节(float32) / 1024 / 1024，单位为 MB\n",
    "\tpara_size = para_num * 4 / 1024 / 1024\n",
    "\treturn para_size\n",
    "\n",
    "get_model_size(c)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d696f3826b7c7df140d5da393a635d37b06efd5c1eee0ae51f5f6acb9f78285"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
