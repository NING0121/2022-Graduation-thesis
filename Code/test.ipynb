{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from Utils import VariantWordDataset\n",
    "\n",
    "train_set = VariantWordDataset(\"train\", \"../Data/source_vocal.pkl\", \"../Data/target_vocal.pkl\")\n",
    "print(train_set.__len__())\n",
    "train_set.__getitem__(1)\n",
    "len(train_set.target_dic.word2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "a = torch.Tensor([[   1,    10],\n",
    "        [ 122, 2138],\n",
    "        [3420, 3560],\n",
    "        [3155, 1954],\n",
    "        [1897,  835],\n",
    "        [ 453, 1366],\n",
    "        [1881,    5],\n",
    "        [1938, 2084],\n",
    "        [3557,  837],\n",
    "        [1711, 1504],\n",
    "        [  41, 3202],\n",
    "        [1377,  716],\n",
    "        [2351, 3359]])\n",
    "b = torch.ones(a.shape[0])\n",
    "# print(b)\n",
    "b = b.unsqueeze(1)\n",
    "a = torch.cat([a,b],1)\n",
    "a\n",
    "length = [2,1,1,1,1, 1,1,1,1,1, 1,1,1]\n",
    "b = []\n",
    "for i,j in zip(a,length):\n",
    "        print(i[:j])\n",
    "        b.append(i[:j].flip(0))\n",
    "        print(b)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# from torch.utils.data.dataloader import default_collate\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(tgt_seq_len, device):\n",
    "\n",
    "    mask = (torch.triu(torch.ones((tgt_seq_len, tgt_seq_len), device=device)) == 1).transpose(0, 1)\n",
    "\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "\n",
    "    return mask\n",
    "\n",
    "def create_mask( src, tgt, device='cpu'):\n",
    "    src_seq_len = src.shape[0]\n",
    "\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    # Decoder的注意力Mask输入，用于掩盖当前position之后的position，所以这里是一个对称矩阵\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len, device)  # [tgt_len,tgt_len]\n",
    "\n",
    "    # Encoder的注意力Mask输入，这部分其实对于Encoder来说是没有用的，所以这里全是0\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
    "\n",
    "    # 用于mask掉Encoder的Token序列中的padding部分,[batch_size, src_len]\n",
    "    src_padding_mask = (src == 0).transpose(0, 1)\n",
    "\n",
    "    # 用于mask掉Decoder的Token序列中的padding部分,batch_size, tgt_len\n",
    "    tgt_padding_mask = (tgt == 0).transpose(0, 1)\n",
    "\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=2, collate_fn=train_set.generate_batch,\n",
    "                                               shuffle=False)\n",
    "for src, tgt in train_loader:\n",
    "    print(src)\n",
    "    print(src.shape)\n",
    "    src = src.transpose(0,1)\n",
    "    print(src)\n",
    "    print(src.shape)\n",
    "    # tgt_input = tgt[:-1, :]\n",
    "\n",
    "    # tgt_out = tgt[1:, :]\n",
    "\n",
    "    # src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "    \n",
    "    # print(\"src shape:\", src.shape)  # [de_tensor_len,batch_size]\n",
    "\n",
    "    # print(\"src_padding_mask shape (batch_size, src_len): \", src_padding_mask.shape)\n",
    "\n",
    "    # print(\"tgt input shape:\", tgt_input.shape)\n",
    "\n",
    "    # print(\"tgt_padding_mask shape: (batch_size, tgt_len) \", tgt_padding_mask.shape)\n",
    "    # print(\"tgt output shape:\", tgt_out.shape)\n",
    "\n",
    "    # print(\"tgt_mask shape (tgt_len,tgt_len): \", tgt_mask.shape)\n",
    "    # print(\"src_mask shape (tgt_len,tgt_len): \", src_mask.shape)\n",
    "    # print(\"src_padding_mask shape (batch_size, src_len): \", src_padding_mask.shape)\n",
    "\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model.TranslationModel import TranslationModel\n",
    "from Model.config import Config\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from Utils.Dictionary import Dictionary\n",
    "\n",
    "config = Config()\n",
    "dic = Dictionary.load_from_file(config.target_dic_path)\n",
    "\n",
    "translation_model = TranslationModel(src_vocab_size=len(train_set.source_dic.word2idx),\n",
    "                                         tgt_vocab_size=len(train_set.target_dic.word2idx),\n",
    "                                         d_model=config.d_model,\n",
    "                                         nhead=config.num_head,\n",
    "                                         num_encoder_layers=config.num_encoder_layers,\n",
    "                                         num_decoder_layers=config.num_decoder_layers,\n",
    "                                         dim_feedforward=config.dim_feedforward,\n",
    "                                         dropout=config.dropout)\n",
    "\n",
    "def translate(tensor):\n",
    "    result = [dic.idx2word[i] for i in tensor.numpy()]\n",
    "    return result\n",
    "\n",
    "\n",
    "for src, tgt in train_loader:\n",
    "    src = src.to(config.device)  # [src_len, batch_size]\n",
    "    tgt = tgt.to(config.device)\n",
    "    tgt_input = tgt[:-1, :]  # 解码部分的输入, [tgt_len,batch_size]\n",
    "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask \\\n",
    "    = create_mask(src, tgt_input, config.device)\n",
    "    logits = translation_model(\n",
    "    src=src,  # Encoder的token序列输入，[src_len,batch_size]\n",
    "    tgt=tgt_input,  # Decoder的token序列输入,[tgt_len,batch_size]\n",
    "    src_mask=src_mask,  # Encoder的注意力Mask输入，这部分其实对于Encoder来说是没有用的\n",
    "    tgt_mask=tgt_mask,\n",
    "    # Decoder的注意力Mask输入，用于掩盖当前position之后的position [tgt_len,tgt_len]\n",
    "    src_key_padding_mask=src_padding_mask,  # 用于mask掉Encoder的Token序列中的padding部分\n",
    "    tgt_key_padding_mask=tgt_padding_mask,  # 用于mask掉Decoder的Token序列中的padding部分\n",
    "    memory_key_padding_mask=src_padding_mask)  # 用于mask掉Encoder的Token序列中的padding部分\n",
    "\n",
    "    tgt_out = tgt[1:, :]  # 解码部分的真实值  shape: [tgt_len,batch_size]\n",
    "    # logits 输出shape为[tgt_len,batch_size,tgt_vocab_size]\n",
    "    # bleu_score(tgt_out.t().squeeze(), tgt_input.t().squeeze())\n",
    "\n",
    "\n",
    "    print(logits.reshape(-1, logits.shape[-1]))\n",
    "    # for i, j in zip(tgt_out.t(), tgt_input.t()):\n",
    "    #     i = [translate(i)]\n",
    "    #     j = [[translate(j)]]\n",
    "    #     print(i)\n",
    "    #     print(j)\n",
    "    #     print(bleu_score(i, j))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from config import Config\n",
    "from Model.TranslationModel import TranslationModel\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "def accuracy(logits, y_true, PAD_IDX):\n",
    "    \"\"\"\n",
    "    :param logits:  [tgt_len,batch_size,tgt_vocab_size]\n",
    "    :param y_true:  [tgt_len,batch_size]\n",
    "    :param PAD_IDX:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    y_pred = logits.transpose(0, 1).argmax(axis=2).reshape(-1)\n",
    "    # 将 [tgt_len,batch_size,tgt_vocab_size] 转成 [batch_size, tgt_len,tgt_vocab_size]\n",
    "    y_true = y_true.transpose(0, 1).reshape(-1)\n",
    "    # 将 [tgt_len,batch_size] 转成 [batch_size， tgt_len]\n",
    "    acc = y_pred.eq(y_true)  # 计算预测值与正确值比较的情况\n",
    "    mask = torch.logical_not(y_true.eq(PAD_IDX))  # 找到真实标签中，mask位置的信息。 mask位置为FALSE，非mask位置为TRUE\n",
    "    acc = acc.logical_and(mask)  # 去掉acc中mask的部分\n",
    "    correct = acc.sum().item()\n",
    "    total = mask.sum().item()\n",
    "    return float(correct) / total, correct, total\n",
    "\n",
    "def train_model(config):\n",
    "    logging.info(\"############载入数据集############\")\n",
    "\n",
    "    train_set = VariantWordDataset(\"train\", \"../Data/source_vocal.pkl\", \"../Data/target_vocal.pkl\")\n",
    "    valid_set = VariantWordDataset(\"test\", \"../Data/source_vocal.pkl\", \"../Data/target_vocal.pkl\")\n",
    "\n",
    "\n",
    "    logging.info(\"############划分数据集############\")\n",
    "\n",
    "    train_set_loader = DataLoader(train_set, batch_size=config.batch_size, \n",
    "                                    collate_fn=train_set.generate_batch, shuffle=False)\n",
    "\n",
    "    valid_set_loader = DataLoader(valid_set, batch_size=config.batch_size, \n",
    "                                    collate_fn=valid_set.generate_batch, shuffle=False)\n",
    "\n",
    "\n",
    "    logging.info(\"############初始化模型############\")\n",
    "    translation_model = TranslationModel(src_vocab_size=len(train_set.source_dic.word2idx),\n",
    "                                         tgt_vocab_size=len(train_set.target_dic.word2idx),\n",
    "                                         d_model=config.d_model,\n",
    "                                         nhead=config.num_head,\n",
    "                                         num_encoder_layers=config.num_encoder_layers,\n",
    "                                         num_decoder_layers=config.num_decoder_layers,\n",
    "                                         dim_feedforward=config.dim_feedforward,\n",
    "                                         dropout=config.dropout)\n",
    "\n",
    "    \n",
    "\n",
    "    model_save_path = os.path.join(config.model_save_dir, 'model.pkl')\n",
    "\n",
    "    if os.path.exists(model_save_path):\n",
    "        loaded_paras = torch.load(model_save_path)\n",
    "        translation_model.load_state_dict(loaded_paras)\n",
    "        logging.info(\"#### 成功载入已有模型，进行追加训练...\")\n",
    "\n",
    "\n",
    "    translation_model = translation_model.to(config.device)\n",
    "\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=train_set.PAD_IDX)\n",
    "    optimizer = torch.optim.Adam(translation_model.parameters(),\n",
    "                                 lr=0.,\n",
    "                                 betas=(config.beta1, config.beta2), eps=config.epsilon)\n",
    "\n",
    "    # lr_scheduler = CustomSchedule(config.d_model, optimizer=optimizer)\n",
    "\n",
    "    translation_model.train()\n",
    "    for epoch in  range(config.epochs):\n",
    "        losses = 0\n",
    "        start_time = time.time()\n",
    "        for idx, (src, tgt) in tqdm(enumerate(train_set_loader)):\n",
    "            src = src.to(config.device)  # [src_len, batch_size]\n",
    "            tgt = tgt.to(config.device)\n",
    "            tgt_input = tgt[:-1, :]  # 解码部分的输入, [tgt_len,batch_size]\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask \\\n",
    "                = create_mask(src, tgt_input, config.device)\n",
    "            logits = translation_model(\n",
    "                src=src,  # Encoder的token序列输入，[src_len,batch_size]\n",
    "                tgt=tgt_input,  # Decoder的token序列输入,[tgt_len,batch_size]\n",
    "                src_mask=src_mask,  # Encoder的注意力Mask输入，这部分其实对于Encoder来说是没有用的\n",
    "                tgt_mask=tgt_mask,\n",
    "                # Decoder的注意力Mask输入，用于掩盖当前position之后的position [tgt_len,tgt_len]\n",
    "                src_key_padding_mask=src_padding_mask,  # 用于mask掉Encoder的Token序列中的padding部分\n",
    "                tgt_key_padding_mask=tgt_padding_mask,  # 用于mask掉Decoder的Token序列中的padding部分\n",
    "                memory_key_padding_mask=src_padding_mask)  # 用于mask掉Encoder的Token序列中的padding部分\n",
    "            # logits 输出shape为[tgt_len,batch_size,tgt_vocab_size]\n",
    "            optimizer.zero_grad()\n",
    "            tgt_out = tgt[1:, :]  # 解码部分的真实值  shape: [tgt_len,batch_size]\n",
    "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "            # [tgt_len*batch_size, tgt_vocab_size] with [tgt_len*batch_size, ]\n",
    "            loss.backward()\n",
    "            # lr_scheduler.step()\n",
    "            optimizer.step()\n",
    "            losses += loss.item()\n",
    "            acc, _, _ = accuracy(logits, tgt_out, train_set.PAD_IDX)\n",
    "            msg = f\"Epoch: {epoch}, Batch[{idx}/{len(train_set_loader)}], Train loss :{loss.item():.3f}, Train acc: {acc}\"\n",
    "            logging.info(msg)\n",
    "        end_time = time.time()\n",
    "        train_loss = losses / len(train_set_loader)\n",
    "        msg = f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Epoch time = {(end_time - start_time):.3f}s\"\n",
    "        logging.info(msg)\n",
    "\n",
    "\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "            acc = evaluate(config, valid_set_loader, translation_model, dataset)\n",
    "            logging.info(f\"Accuracy on validation{acc:.3f}\")\n",
    "            torch.save(translation_model.state_dict(), model_save_path)\n",
    "\n",
    "\n",
    "def evaluate(config, valid_iter, model, dataset):\n",
    "    model.eval()\n",
    "    correct, totals = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (src, tgt) in enumerate(valid_iter):\n",
    "            src = src.to(config.device)\n",
    "            tgt = tgt.to(config.device)\n",
    "            tgt_input = tgt[:-1, :]  # 解码部分的输入\n",
    "\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = \\\n",
    "                create_mask(src, tgt_input, device=config.device)\n",
    "\n",
    "            logits = model(src=src,  # Encoder的token序列输入，\n",
    "                           tgt=tgt_input,  # Decoder的token序列输入\n",
    "                           src_mask=src_mask,  # Encoder的注意力Mask输入，这部分其实对于Encoder来说是没有用的\n",
    "                           tgt_mask=tgt_mask,  # Decoder的注意力Mask输入，用于掩盖当前position之后的position\n",
    "                           src_key_padding_mask=src_padding_mask,  # 用于mask掉Encoder的Token序列中的padding部分\n",
    "                           tgt_key_padding_mask=tgt_padding_mask,  # 用于mask掉Decoder的Token序列中的padding部分\n",
    "                           memory_key_padding_mask=src_padding_mask)  # 用于mask掉Encoder的Token序列中的padding部分\n",
    "            tgt_out = tgt[1:, :]  # 解码部分的真实值  shape: [tgt_len,batch_size]\n",
    "            _, c, t = accuracy(logits, tgt_out, valid_iter.PAD_IDX)\n",
    "            correct += c\n",
    "            totals += t\n",
    "    model.train()\n",
    "    return float(correct) / totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from Model.config import Config\n",
    "config = Config()\n",
    "train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from Model.TranslationModel import TranslationModel\n",
    "from Model.config import Config\n",
    "import torch\n",
    "config = Config()\n",
    "translation_model = TranslationModel(src_vocab_size=config.source_vocab_size,\n",
    "                                        tgt_vocab_size=config.target_vocab_size,\n",
    "                                        d_model=config.d_model,\n",
    "                                        nhead=config.num_head,\n",
    "                                        num_encoder_layers=config.num_encoder_layers,\n",
    "                                        num_decoder_layers=config.num_decoder_layers,\n",
    "                                        dim_feedforward=config.dim_feedforward,\n",
    "                                        dropout=config.dropout)\n",
    "\n",
    "load_param = torch.load(\"../cache/model.pkl\", map_location=torch.device('cpu'))\n",
    "translation_model.load_state_dict(load_param)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.Dictionary import Dictionary\n",
    "dic = Dictionary.load_from_file(\"../Data/source_vocal.pkl\")\n",
    "len(dic.word2idx)\n",
    "dic.word2idx\n",
    "dic.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchtext.data.metrics import bleu_score\n",
    "# candidate_corpus = [['权', '益', '服', '务', ':', '您', '领', '取', '的', '双', '突', '战', '法', '指']]\n",
    "# references_corpus = [[['权', '益', '服', '务', ':', '您', '领', '取', '的', '双', '突', '战', '法']]]\n",
    "# candidate_corpus = ['权', '益', '服', '务']\n",
    "# references_corpus = ['权', '益', '服', '务']\n",
    "candidate_corpus = [['权', '益', '服', '务', ':', '您', '领', '取', '的', '双', '突', '战', '法', '指', '标', '今', '天', '已', '有', '信', '号', '提', '醒', '啦', ',', '快', '登', '录', '大', '智', '慧', 'b', '查', '看', '或', '现', '在', '打', '开', '链', '接', 'h', ':', '/', '/', 't', '.', 'v', '.', 'e', '.', 'c', 'd', '/', 'q', '使', '用', '.', '[EOS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']]\n",
    "references_corpus = [[['权', '益', '服', '务', ':', '您', '领', '取', '的', '双', '突', '战', '法', '指', '标', '今', '天', '已', '有', '信', '号', '提', '醒', '啦', ',', '快', '登', '录', '大', '智', '慧', 'b', '查', '看', '或', '现', '在', '打', '开', '链', '接', 'h', ':', '/', '/', 't', '.', 'v', '.', 'e', '.', 'c', 'd', '/', 'm', '使', '用', '.', '[EOS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']]]\n",
    "\n",
    "print(len(candidate_corpus[0]))\n",
    "print(len(references_corpus[0]))\n",
    "print(references_corpus[0][0])\n",
    "bleu_score(candidate_corpus, references_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "a = torch.Tensor([[10,11,12,13,14,15],[10,11,12,13,14,15]])\n",
    "a.shape\n",
    "a.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "a = torch.Tensor([[1],[2],[3]])\n",
    "b = torch.Tensor([[1],[2],[3]])\n",
    "torch.cat((a,b), 1)\n",
    "a.shape[0]\n",
    "a.size(0)\n",
    "torch.zeros(8,1,device=\"cpu\")\n",
    "a = torch.Tensor([[0.3, 0.7]])\n",
    "F.softmax(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算原始数据集的BLEU分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dictionary from ../Data/source_vocal.pkl\n",
      "loading dictionary from ../Data/target_vocal.pkl\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "from data_loader import Dictionary\n",
    "from nltk.translate import bleu_score\n",
    "\n",
    "source_dic =Dictionary.load_from_file(\"../Data/source_vocal.pkl\")\n",
    "target_dic =Dictionary.load_from_file(\"../Data/target_vocal.pkl\")\n",
    "\n",
    "\n",
    "test_data = pd.read_csv(\"../Data/Dataset/test_data.csv\",index_col=0)\n",
    "raw_data = test_data[\"raw_data\"].tolist()\n",
    "right_data = test_data[\"right_data\"].tolist()\n",
    "\n",
    "# def replace(sentence):\n",
    "\n",
    "def C_trans_to_E(string):\n",
    "    E_pun = u',.!?[]()<>\"\\':;'\n",
    "    C_pun = u'，。！？【】（）《》“‘：；'\n",
    "    table= {ord(f):ord(t) for f,t in zip(C_pun,E_pun)}\n",
    "    return string.translate(table)\n",
    "\n",
    "\n",
    "raw_data2idx = []\n",
    "right_data2idx = []\n",
    "\n",
    "# 直接构建\n",
    "for source, target in zip(raw_data, right_data):\n",
    "    raw_data2idx.append(list(source))\n",
    "    right_data2idx.append([list(target)])\n",
    "\n",
    "bleuscore_1 =  bleu_score.corpus_bleu(right_data2idx, raw_data2idx)\n",
    "\n",
    "raw_data2idx.clear()\n",
    "right_data2idx.clear()\n",
    "\n",
    "# C2E构建\n",
    "for source, target in zip(raw_data, right_data):\n",
    "    raw_data2idx.append(list(C_trans_to_E(source)))\n",
    "    right_data2idx.append([list(C_trans_to_E(target))])\n",
    "bleuscore_2=  bleu_score.corpus_bleu(right_data2idx, raw_data2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "直接构建BLEU_SCORE: 0.6048509912936112\n",
      "C2E构建BLEU_SCORE: 0.7032748700029685\n"
     ]
    }
   ],
   "source": [
    "print(\"直接构建BLEU_SCORE:\", bleuscore_1)\n",
    "print(\"C2E构建BLEU_SCORE:\", bleuscore_2)\n",
    "# print(\"C2E构建BLEU_SCORE:\", bleuscore_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "昨晚开出(46_虎),您鈡了嚒?加wei:18683367080 我可以告诉你下次的壹逍叁玛,早加就知道\n"
     ]
    }
   ],
   "source": [
    "def E_trans_to_C(string):\n",
    "    E_pun = u',.!?[]()<>\"\\':;'\n",
    "    C_pun = u'，。！？【】（）《》“‘：；'\n",
    "    table= {ord(f):ord(t) for f,t in zip(C_pun,E_pun)}\n",
    "    return string.translate(table)\n",
    "\n",
    "test = \"昨晚开出(46_虎)，您鈡了嚒?加wei：18683367080 我可以告诉你下次的壹逍叁玛，早加就知道\"\n",
    "print(E_trans_to_C(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('wnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6289b70d6cb9962ea39d867a2ca8eb472c1f05613c49f546e0107839d378edad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
